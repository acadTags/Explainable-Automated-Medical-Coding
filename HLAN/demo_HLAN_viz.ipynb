{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Automated Medical Coding\n",
    "An demo of using Hierarchical Label-wise Attention Network (HLAN) for explainable medical coding.\n",
    "\n",
    "The demo allows to input a discharge summary and then predict the ICD-9 codes related to it. \n",
    "* [0. Preparation](#0.-Preparation): import libraries\n",
    "* [1. Configuration](#1.-Configuration): setting hyper-parameters\n",
    "* [2. Data preprocessing](#2.-Data-preprocessing): (i) vocabulary building; (ii) sentence parsing, tokenisation\n",
    "* [3. Prediction and visualistion](#3.-Prediction-and-visualisation): (i) loading a pre-trained HLAN model (from the MIMIC-III dataset) to predict the top 50 ICD-9 codes; (ii) visualising the word-level and sentence-level attention scores for each assigned ICD-9 code.\n",
    "\n",
    "The program does not need GPU. On a CPU, it takes around 4s (seconds) to load the model to predict, and only a further 10s to visualise a batch (by default, 32) of discharge summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preparation\n",
    "Import libraries to use for preprocessing data, loading a deep learning model, prediction, and visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages if running this notebook for the first time\n",
    "import sys;print(sys.version)\n",
    "\n",
    "#!pip install --upgrade pip\n",
    "#!pip install gensim\n",
    "#!pip install tflearn\n",
    "#!pip install pandas\n",
    "#!pip install scikit-learn\n",
    "#!pip install tqdm\n",
    "#!pip install nltk\n",
    "#!pip install spacy==2.3.2\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import warnings\n",
    "#do not show warning messages while importing tensorflow and functions that use numpy\n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    import tensorflow as tf\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "    #tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # or any {DEBUG, INFO, WARN, ERROR, FATAL}\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\",category=RuntimeWarning)\n",
    "    from data_util_gensim import load_data_multilabel_pre_split_for_pred,create_vocabulary,create_vocabulary_label_for_predict,get_label_sim_matrix,get_label_sub_matrix\n",
    "    from model_predict_util import preprocessing,viz_attention_scores,retrieve_icd_descs,output_to_file,display_for_qualitative_evaluation,display_for_qualitative_evaluation_per_label\n",
    "\n",
    "import time\n",
    "import pickle    \n",
    "import pandas as pd\n",
    "\n",
    "from HAN_model_dynamic import HAN    \n",
    "from tflearn.data_utils import pad_sequences\n",
    "from gensim.models import Word2Vec            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration\n",
    "The setting and hyper-parameters for the HLAN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two key settings\n",
    "# (i) whether to input a document (otherwise, using a .txt file of documents, where each document occupies one line)\n",
    "to_input = True\n",
    "\n",
    "# (ii) the model checkpoint folder to load from - choose one from the options below\n",
    "#HLAN+LE+sent split trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_per_label_bs32_sent_split_LE/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=True #HLAN trained on MIMIC-III-50\n",
    "\n",
    "#HLAN+LE trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_per_label_bs32_LE/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=False #HLAN trained on MIMIC-III-50\n",
    "\n",
    "#HLAN+LE+sent split trained on MIMIC-III-shielding\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_per_label_bs32_sent_split_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=True #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HLAN+LE trained on MIMIC-III-shielding\n",
    "ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_per_label_bs32_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 32;per_label_attention=True;per_label_sent_only=False;sent_split=False #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HA-GRU trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_per_label_sent_only_bs32_LE/\";dataset = \"mimic3-ds-50\";batch_size = 32;per_label_attention=True;per_label_sent_only=True;sent_split=False #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HA-GRU trained on MIMIC-III-shielding\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_per_label_sent_only_bs32_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 32;per_label_attention=True;per_label_sent_only=True;sent_split=False #HLAN trained on MIMIC-III-shielding\n",
    "\n",
    "#HAN+sent split trained on MIMIC-III\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_sent_split_LE/\";dataset = \"mimic3-ds\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=False #HAN trained on MIMIC-III\n",
    "\n",
    "#HAN trained on MIMIC-III\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_LE/\";dataset = \"mimic3-ds\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=True\n",
    "\n",
    "#HAN+sent split trained on MIMIC-III-50\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_50_LE/\";dataset = \"mimic3-ds-50\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=False\n",
    "\n",
    "#HAN+sent split trained on MIMIC-III-shielding\n",
    "#ckpt_dir=\"../checkpoints/checkpoint_HAN_shielding_th50_LE/\";dataset = \"mimic3-ds-shielding-th50\";batch_size = 128;per_label_attention=False;per_label_sent_only=False;sent_split=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other settings and hyper-parameters\n",
    "word2vec_model_path = \"../embeddings/processed_full.w2v\"\n",
    "emb_model_path = \"../embeddings/word-mimic3-ds-label.model\" #using the one learned from the full label sets of mimic-iii discharge summaries\n",
    "label_embedding_model_path = \"../embeddings/code-emb-mimic3-tr-400.model\" # for label embedding initialisation (W_projection)\n",
    "label_embedding_model_path_per_label = \"../embeddings/code-emb-mimic3-tr-200.model\" # for label embedding initialisation (per_label context_vectors)\n",
    "kb_icd9 = \"../knowledge_bases/kb-icd-sub.csv\"\n",
    "\n",
    "gpu=True\n",
    "learning_rate = 0.01\n",
    "decay_steps = 6000\n",
    "decay_rate = 1.0\n",
    "sequence_length = 2500\n",
    "num_sentences = 100\n",
    "embed_size=100\n",
    "hidden_size=100\n",
    "is_training=False\n",
    "lambda_sim=0.0\n",
    "lambda_sub=0.0\n",
    "dynamic_sem=True\n",
    "dynamic_sem_l2=False\n",
    "multi_label_flag=True\n",
    "pred_threshold=0.5\n",
    "use_random_sampling=False\n",
    "miu_factor=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing\n",
    "Part1: Loading label vocabulary and building word vocabulary from pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using gpu or not\n",
    "if not gpu: \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  \n",
    "    \n",
    "#load the label list\n",
    "vocabulary_word2index_label,vocabulary_index2word_label = create_vocabulary_label_for_predict(name_scope=dataset + \"-HAN\") # keep a distinct name scope for each model and each dataset.\n",
    "if vocabulary_word2index_label == None:\n",
    "    print('_label_vocabulary.pik file unavailable')\n",
    "    sys.exit()\n",
    "\n",
    "#get the number of labels\n",
    "num_classes=len(vocabulary_word2index_label)\n",
    "\n",
    "#building the vocabulary list from the pre-trained word embeddings\n",
    "vocabulary_word2index, vocabulary_index2word = create_vocabulary(word2vec_model_path,name_scope=dataset + \"-HAN\")\n",
    "vocab_size = len(vocabulary_word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part2: Input a discharge summary and preprocess the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#input or use files and preprocess a discharge summary\n",
    "if to_input:\n",
    "    print('Please input a discharge summary.');time.sleep(0.1) #sleep for proper display\n",
    "    report_raw = input(); preprocess = True\n",
    "\n",
    "    filename_to_predict = 'raw dis sum input.txt'\n",
    "    output_to_file(\"../datasets/%s\" % filename_to_predict,report_raw)\n",
    "else:\n",
    "    # directly load from file (each preprocessed document in a row)\n",
    "    if not sent_split:\n",
    "        #the sample document in the MIMIC-III-50\n",
    "        #filename_to_predict = 'sample MIMIC-III-50 test doc-24.txt'\n",
    "        #the 1730 testing documents for MIMIC-III-50\n",
    "        filename_to_predict= 'mimiciii_test_50_th0.txt'\n",
    "    else:   \n",
    "        #the sample sentence split document\n",
    "        #filename_to_predict='sample MIMIC-III-50 test doc-24 sent split.txt'\n",
    "        #the 1730 testing documents, all sentence-splitted, for MIMIC-III-50\n",
    "        filename_to_predict= 'mimiciii_test_50_sent_split_th0_for_HAN.txt'\n",
    "    preprocess = False # no further preprocessing if loading preprocessed documents\n",
    "    \n",
    "testing_data_path = \"../datasets/%s\" % filename_to_predict\n",
    "\n",
    "if preprocess:\n",
    "    #preprocess data (sentence parsing and tokenisation)\n",
    "    #this is only used for a *raw* discharge summary (one each time) and when to_input is set as true.\n",
    "    clinical_note_preprocessed_str = preprocessing(raw_clinical_note_file=testing_data_path,sent_parsing=sent_split,num_of_sen=100,num_of_sen_len=25) # tokenisation, padding, lower casing, sentence splitting\n",
    "    output_to_file('clinical_note_temp.txt',clinical_note_preprocessed_str) #load the preprocessed data\n",
    "    testX, testY = load_data_multilabel_pre_split_for_pred(vocabulary_word2index,vocabulary_word2index_label,data_path='clinical_note_temp.txt')\n",
    "else:\n",
    "    #this allows processing many preprocessed documents together, each in a row of the file in the testing_data_path\n",
    "    testX, testY = load_data_multilabel_pre_split_for_pred(vocabulary_word2index,vocabulary_word2index_label,data_path=testing_data_path)\n",
    "\n",
    "#padding to the maximum sequence length\n",
    "testX = pad_sequences(testX, maxlen=sequence_length, value=0.)  # padding to max length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prediction and visualisation\n",
    "3.1 Load HLAN model to predict ICD-9 code\n",
    "\n",
    "<img src=\"HLAN-architecture.PNG\" width=\"600\" height=\"300\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "#create session.\n",
    "config=tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=False\n",
    "with tf.Session(config=config) as sess:\n",
    "    #Instantiate Model\n",
    "    model=HAN(num_classes, learning_rate, batch_size, decay_steps, decay_rate,sequence_length,num_sentences,vocab_size,embed_size,hidden_size,is_training,lambda_sim,lambda_sub,dynamic_sem,dynamic_sem_l2,per_label_attention,per_label_sent_only,multi_label_flag=multi_label_flag)\n",
    "    saver=tf.train.Saver(max_to_keep = 1) # only keep the latest model, here is the best model\n",
    "    if os.path.exists(ckpt_dir+\"checkpoint\"):\n",
    "        print(\"Restoring Variables from Checkpoint\")\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(ckpt_dir))\n",
    "    else:\n",
    "        print(\"Can't find the checkpoint.going to stop\")\n",
    "        sys.exit()\n",
    "\n",
    "    #get prediction results and attention scores\n",
    "    if per_label_attention: # to do for per_label_sent_only\n",
    "        prediction_str = display_for_qualitative_evaluation_per_label(sess,model,testX,testY,batch_size,vocabulary_index2word,vocabulary_index2word_label,sequence_length,per_label_sent_only,num_sentences=num_sentences,threshold=pred_threshold,use_random_sampling=use_random_sampling,miu_factor=miu_factor) \n",
    "    else:\n",
    "        prediction_str = display_for_qualitative_evaluation(sess,model,testX,testY,batch_size,vocabulary_index2word,vocabulary_index2word_label,sequence_length=sequence_length,num_sentences=num_sentences,threshold=pred_threshold,use_random_sampling=use_random_sampling,miu_factor=miu_factor)\n",
    "\n",
    "#prediction_str #to display raw attention score outputs with predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Display and save the label-wise attention visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit() # just to get testing time, not visualise the predictions\n",
    "#print(len(prediction_str))\n",
    "#get attention score and labels for visualisation\n",
    "list_doc_label_marks,list_doc_att_viz,dict_doc_pred = viz_attention_scores(prediction_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(list_doc_att_viz) == 0: # if no ICD code assigned for the document.\n",
    "    print('No ICD code predicted for this document.')    \n",
    "else:    \n",
    "    for ind, (doc_label_mark, doc_att_viz) in enumerate(zip(list_doc_label_marks,list_doc_att_viz)):\n",
    "        # retrieve and display ICD-9 codes and descriptions \n",
    "        doc_label_mark_ele_list = doc_label_mark.split('-')\n",
    "        if len(doc_label_mark_ele_list)==2: # HAN model, same visualisation for all codes\n",
    "            doc_label_mark_without_code = '-'.join(doc_label_mark_ele_list[:3])\n",
    "            print(doc_label_mark_without_code)\n",
    "            filename = 'att-%s.xlsx' % (doc_label_mark[:len(doc_label_mark)-1])     \n",
    "            predictions = dict_doc_pred[doc_label_mark_without_code]\n",
    "            predictions = predictions.split('labels:')[0]\n",
    "            ICD_9_codes = predictions.split(' ')[1:]\n",
    "            print('Predicted code list:')\n",
    "            for ICD_9_code in ICD_9_codes:\n",
    "                # retrieve the short title and the long title of this ICD-9 code\n",
    "                _, long_tit,code_type = retrieve_icd_descs(ICD_9_code)\n",
    "                print(code_type,'code:',ICD_9_code,'(',long_tit,')')\n",
    "        else: # HLAN or HA-GRU, a different visualisation for each label\n",
    "            ICD_9_code = doc_label_mark_ele_list[3] # retrieve the predicted ICD-9 code\n",
    "            ICD_9_code = ICD_9_code[:len(ICD_9_code)-1] # drop the trailing colon\n",
    "            short_tit, long_tit,code_type = retrieve_icd_descs(ICD_9_code) # retrieve the short title and the long title of this ICD-9 code\n",
    "            doc_label_mark_without_code = '-'.join(doc_label_mark_ele_list[:3])\n",
    "            print(doc_label_mark_without_code,'to predict %s code' % code_type,ICD_9_code,'(%s)' % (long_tit))\n",
    "            filename = 'att-%s(%s).xlsx' % (doc_label_mark[:len(doc_label_mark)-1],short_tit) #do not include the colon in the last char\n",
    "            filename = filename.replace('/','').replace('<','').replace('>','') # avoid slash / or <, > signs in the filename\n",
    "        \n",
    "        # export the visualisation to an Excel sheet\n",
    "        filename = '..\\explanations\\\\' + filename # put the files under the ..\\explanations\\ folder.\n",
    "        doc_att_viz.set_properties(**{'font-size': '9pt'})\\\n",
    "                   .to_excel(filename, engine='openpyxl')\n",
    "        print('Visualisation below saved to %s.' % filename) \n",
    "        \n",
    "        # reset the font for the display below\n",
    "        doc_att_viz.set_properties(**{'font-size': '5pt'})\n",
    "        display(doc_att_viz)\n",
    "        \n",
    "        #display the prediction when the label-wise visualisations for the document end\n",
    "        if ind!=len(list_doc_label_marks)-1:\n",
    "            #this is not the last doc label mark\n",
    "            if list_doc_label_marks[ind+1][:len(doc_label_mark_without_code)] != doc_label_mark_without_code:                \n",
    "                #the next doc label mark is not the current one\n",
    "                print(dict_doc_pred[doc_label_mark_without_code])\n",
    "                #print('Visualisation for %s ended.\\n' % doc_label_mark_without_code)\n",
    "        else:\n",
    "            #this is the last doc label mark\n",
    "            print(dict_doc_pred[doc_label_mark_without_code])                                   \n",
    "            #print('Visualisation for %s ended.\\n' % doc_label_mark_without_code)\n",
    "\n",
    "print(\"--- The prediction and visualisation took %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
